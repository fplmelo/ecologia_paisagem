{
  "hash": "43bcb58a2243211a62559df479e972b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 6 - Mean Tests and Design of Experiments\"\nsubtitle: \"RMDA ARES40011\"\nauthor: \"Felipe Melo\"\ndate: \"last-modified\"\ndate-format: \"long\"\nfilters:\n  - naquiz\nformat:\n  html:\n    toc: true\n    toc-expanded: 3\neditor: \n  markdown: \n    wrap: sentence\n---\n![](/img/mean_test.jpeg){width=\"500\"}\n\n[Artwork by \\@allison_horst](https://twitter.com/allison_horst){style=\"text-align: center\"}\n\n# Preparation\n\n## Learning objectives\n\n| Research Methods | Data Analyses |\n|------------------------------------|------------------------------------|\n| Types of experimental designs | Understand \"mean tests\" |\n| Error Type I and II |Parametric vs. non-parametric mean tests |\n|  | Data visualization for means|\n\n\n**For Data Analyses**\n\n-   {{< fa book >}} [Check chapter this e-book, R Notes](https://brad-cannell.github.io/r_notes/contingency-tables.html)\n-   {{< fa book >}} [Cookbook for R](http://www.cookbook-r.com/Manipulating_data/Converting_between_data_frames_and_contingency_tables/)\n-   {{< fa tools >}} [Nice Tutorial for Contignecy Tables](https://sbc.shef.ac.uk/stats-in-r/practical.nb.html)\n-   {{< fa tools >}} [Nice Tutorial for Chi-Square tests](https://www.sthda.com/english/wiki/chi-square-test-of-independence-in-r)\n\n**For Research Methods**\n\n-   {{< fa book >}} [What is replication?](https://en.wikipedia.org/wiki/Replication_(statistics))\n\n-   {{< fa video >}} [Degrees of Freeedom](https://youtu.be/oFz8iOx96mw)\n\n-   {{< fa video >}} [About statistical power and replicates in experimental design](https://youtu.be/sk5ttlX2i5I)\n\n-   {{< fa video >}} [Understanding Replication and Randomization](https://youtu.be/Mufk0ZJVqQY)\n\n-   {{< fa video >}} [Types of experiments](https://youtu.be/DavsDxgYQo4)\n\n# Lesson\n\n# Part 1 - Research Methods\n\n## Types of experimental desing\n\nExperimental design is a crucial process for ensuring that studies are statistically sound and that conclusions drawn from them are reliable. It involves carefully planning how data will be collected to answer a specific question.  Experimental designs are broadly classified into two main categories: \n\n**Single-Factor Experiments** and **Multi-Factor Experiments**.\n\n### Single-Factor Experiments\nThese experiments involve varying only a single factor or treatment, while all other factors are kept constant or applied uniformly to all plots.\n\n**1. Completely Randomized Design (CRD)**\n\n![](/img/crd.png){fig-align=center width=\"300\"}\n\nIn a CRD, treatments are assigned completely at random to experimental units, ensuring each unit has an equal chance of receiving any one treatment.\n\nHomogeneity Requirement: CRD is applicable primarily when the experimental material is homogenous. It is generally used in lab experimental conditions where environmental factors can be easily controlled. Due to the typical heterogeneity of soil in fields, CRD is often not the preferred method for field experiments.\n\nLocal Control: The principle of 'Local-control' is not used in CRD.\n\nAdvantages: CRD is easy to understand, allows for a varying number of replications per treatment, offers high flexibility in the number of treatments used, requires simple statistical analysis, and provides the maximum number of degrees of freedom.\n\n\nDisadvantages: It can only be applied to homogenous experiments and does not incorporate the principle of 'Local-control'.\n\n\n**2. Randomized Block Design (RBD)**\n\n![](/img/rbd.png){fig-align=center width=\"300\"}\n\n\nRBD is the most commonly used experimental design in agriculture. It incorporates 'local-control' by grouping experimental material into homogenous subgroups called blocks. Each block then contains the entire set of treatments, making a block equivalent to a replication.\n\n\nKey Principles: RBD effectively utilizes all three core principles of experimental design: replication, randomization, and local-control.\n\nReplication: In RBD, blocks are repeated across the field, often four to six times, to gather more data from a broader area and increase confidence in the results, such as yield differences.\n\nRandomization: To eliminate bias and avoid unintended benefits from field variability (e.g., a field becoming more productive from east to west), the placement of each treatment within a block is assigned by chance, such as by flipping a coin. This ensures that any observed yield differences are genuinely due to the treatments and not other uncontrolled factors.\n\nAdvantages: RBD is generally more efficient and accurate than CRD, has a lower chance of error, offers high flexibility in the number of treatments and replications, and allows for relatively simple statistical analysis, even if a single value is missing. Errors related to any treatment can be isolated.\n\nDisadvantages: RBD is not recommended for a very large number of treatments because this can increase the size of each block, potentially leading to heterogeneity within blocks and higher experimental errors.\n\n**Other Noteworthy Experimental Designs**\n\n• Randomized Two-Treatment Experiment: Involves randomly assigning individuals to one of two groups, typically an experimental condition (where the hypothesized cause is present) and a control condition (where it's absent or a placebo is given). Random assignment aims to ensure groups are similar in all respects, including pre-existing conditions.\n\n•Rigorously Controlled Design: This design involves carefully assigning subjects to treatment groups to ensure they are similar in important ways. However, it is hard to implement due to the difficulty in identifying all relevant differentiations and should generally be avoided.\n\n• Matched Pairs Design: Treatments are administered to two groups that are matched in certain ways. Examples include measuring the same individual's right arm versus left arm, or conducting before-and-after experiments (e.g., weight before and after a diet).\n\n• Repeated Measures Design: In this design, all participants are exposed to all levels of the independent variable, meaning they experience all conditions. Random assignment occurs for the order in which these conditions are experienced, rather than assigning participants to specific conditions.\n\n![](/img/plant_exp.png){fig-align=center width=\"300\"}\n\n\n:::callout-tip\nAll true experiments, regardless of their specific type, emphasize random assignment to conditions, which is crucial for maximizing internal validity. Replication, the repetition of an experiment on more than one subject, is also vital to ensure the sample is large enough to differentiate true effects from random ones and to allow others to duplicate results.\n:::\n\n# Part 2 - Data Analyses\n\n## Understanding Mean Tests in R\n\nMean tests are statistical procedures used to determine if there are significant differences between the means of two or more groups, or if a sample mean is significantly different from a specific value. These tests are fundamental in validating relationships, especially when dealing with a categorical variable and a continuous variable. The sources highlight various tests, including t-tests, Analysis of Variance (ANOVA), and their non-parametric counterparts, the Wilcoxon rank sum test and Kruskal-Wallis test\n\n### Introduction to Mean Tests\n\nMean tests are fundamental statistical tools used to **determine if there are significant differences between the average values (means) of one or more groups or against a specific hypothesised value**. They are crucial for drawing conclusions from data in various fields, including epidemiology and data science.\n\n### Assumptions for Mean Tests\n\nParametric mean tests, such as the t-test and ANOVA, rely on several key assumptions about the data [5-12]. It is a best practice to **visually check data distribution and run assumption tests prior to performing any statistical tests**.\n\nThe common assumptions for parametric tests are:\n\n*   **Independence**: The samples are independent of each other [5, 6, 10]. For example, in an unpaired t-test, the observations in one group should not be related to the observations in another group. Paired tests, however, are an exception, as they explicitly deal with related or dependent samples.\n\n*   **Normality**: The data for each group or the differences between paired observations should be **approximately normally distributed**.\n \n    *   **Checking normality**: This can be assessed using the **Shapiro-Wilk test** (`shapiro.test()` from `base` R or `shapiro_test()` from `rstatix`), which is suitable for sample sizes between 3 and 5000 observations. A p-value greater than 0.05 from this test indicates that the data distribution is not significantly different from a normal distribution, suggesting normality can be assumed.\n    \n    *   **Visual confirmation**: **Quantile-Quantile (Q-Q) plots** (`ggqqplot()` from `ggpubr`) or **histograms** (`geom_histogram()` with `ggplot`) are useful visual tools to assess normality [19-23]. If points in a Q-Q plot fall approximately along the reference line, normality can be assumed [19]. Statistical tests can sometimes be overly sensitive to large sample sizes or not sensitive enough at low sample sizes, making visual interpretation valuable.\n    \n    *   **Large Sample Sizes**: For sample sizes greater than 30 (n > 30), the **Central Limit Theorem** suggests that violations of the normality assumption should not pose major issues, allowing the use of parametric tests [25]. However, visual checks and the Shapiro-Wilk test can still help identify serious deviations.\n    \n*   **Homoscedasticity (Equality of Variances)**: The variance of the outcome variable should be **equal across the groups** being compared. This assumption is primarily made by the **classic Student's t-test** and standard ANOVA.\n    *   **Checking homoscedasticity**: **Levene's test** (`leveneTest()` from `car` package or `levene_test()` from `rstatix`) is a robust method to compare variances. A p-value greater than 0.05 suggests that the variances are not significantly different, implying homogeneity of variances.\n    *   **Visual confirmation**: Boxplots can also be used to visually examine homoscedasticity.\n    *   **Handling unequal variances**: If variances are unequal, **Welch's t-test** (for two groups) or **Welch's ANOVA** (for more than two groups) are more appropriate alternatives as they do not assume equal variances. R's `t.test()` function defaults to Welch's t-test, making it a safer choice.\n*   **No Significant Outliers**: **Outliers** can skew results and violate assumptions. They can be identified using boxplot methods, such as `identify_outliers()` from the `rstatix` package. If extreme outliers are present, non-parametric tests might be considered.\n\n### Parametric vs Non-Parametric Versions\n\nThe choice between parametric and non-parametric tests largely depends on whether the data meets the assumptions of parametric tests, especially normality and homoscedasticity.\n\n*   **Parametric Tests**:\n    *   Assume specific distributions for the data (e.g., normal distribution) and often require assumptions about the equality of variances.\n    *   Examples include **T-tests** and **Analysis of Variance (ANOVA)**.\n    *   They are generally more powerful when their assumptions are met.\n\n*   **Non-Parametric Tests**:\n    *   Do **not rely on assumptions about the distribution of the data**.\n    *   Often focus on medians or ranks rather than means.\n    *   Examples include the **Wilcoxon test** (Rank-Sum test for two groups, Signed-Rank test for paired samples) and the **Kruskal-Wallis test** (for more than two groups).\n    *   Recommended when parametric test assumptions are violated.\n\n### One-sample comparisons\n\nOne-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).\n\nGenerally, the theoretical mean comes from:\n\n- A previous experiment. For example, compare whether the mean weight of mice differs from 200 mg, a value determined in a previous study.\n\n- From an experiment where you have control and treatment conditions. If you express your data as “percent of control”, you can test whether the average value of treatment condition differs significantly from 100.\n\nIn statistics, we can define the corresponding null hypothesis (H0) as follow:\n\n$H_0 : m=\\mu$\n\n$H_0 : m \\geq \\mu$\n\n$H_0 : m \\leq \\mu$\n\nThe corresponding alternative hypotheses (H') are as follow:\n\n$H': m\\neq\\mu$ (just different) \n\n$H' : m < \\mu$ (smaller) \n\n$H' : m > \\mu$ (greater)\n\n:::callout-tip\nNow, imagine the `palmerpenguins` dataset. Imagine that you are a researcher working with penguins and someone reported a carcase of a dead penguin in an island that where you know that species `Adelie` was never registered. The only precise data you have is the length of the bill found (39.2mm). Can we use a **one-sample test** for that?\n:::\n\nLet's check the mean values of bill length\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins %>% \n  group_by(species) %>% \n  drop_na() %>% \n  summarise(bill_length_mean = mean(bill_length_mm), bill_lengh_sd = sd(bill_length_mm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  species   bill_length_mean bill_lengh_sd\n  <fct>                <dbl>         <dbl>\n1 Adelie                38.8          2.66\n2 Chinstrap             48.8          3.34\n3 Gentoo                47.6          3.11\n```\n\n\n:::\n:::\n\n\nLet's test our possible hypotheses\n\n1- The dead penguin is from **Adelie** species, then $H' : m < \\mu$ where $\\mu$ is the carcase and `m` could be the mean of other penguin species, let's say **Chinstrap**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  filter(species==\"Adelie\") %>% \n  select(bill_length_mm) -> m\nt.test(m, mu = 39.2, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = -1.8852, df = 150, p-value = 0.06134\nalternative hypothesis: true mean is not equal to 39.2\n95 percent confidence interval:\n 38.36312 39.21966\nsample estimates:\nmean of x \n 38.79139 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = -1.8852, df = 150, p-value = 0.9693\nalternative hypothesis: true mean is greater than 39.2\n95 percent confidence interval:\n 38.43266      Inf\nsample estimates:\nmean of x \n 38.79139 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = -1.8852, df = 150, p-value = 0.03067\nalternative hypothesis: true mean is less than 39.2\n95 percent confidence interval:\n     -Inf 39.15012\nsample estimates:\nmean of x \n 38.79139 \n```\n\n\n:::\n\n```{.r .cell-code}\npenguins %>% \n  filter(species==\"Chinstrap\") %>% \n  select(bill_length_mm) -> m\nt.test(m, mu = 39.2, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 23.79, df = 67, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 39.2\n95 percent confidence interval:\n 48.02555 49.64210\nsample estimates:\nmean of x \n 48.83382 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 23.79, df = 67, p-value < 2.2e-16\nalternative hypothesis: true mean is greater than 39.2\n95 percent confidence interval:\n 48.15841      Inf\nsample estimates:\nmean of x \n 48.83382 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 23.79, df = 67, p-value = 1\nalternative hypothesis: true mean is less than 39.2\n95 percent confidence interval:\n     -Inf 49.50924\nsample estimates:\nmean of x \n 48.83382 \n```\n\n\n:::\n\n```{.r .cell-code}\npenguins %>% \n  filter(species==\"Gentoo\") %>% \n  select(bill_length_mm) -> m\nt.test(m, mu = 39.2, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 29.886, df = 122, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 39.2\n95 percent confidence interval:\n 46.95478 48.05497\nsample estimates:\nmean of x \n 47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 29.886, df = 122, p-value < 2.2e-16\nalternative hypothesis: true mean is greater than 39.2\n95 percent confidence interval:\n 47.04431      Inf\nsample estimates:\nmean of x \n 47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(m, mu = 39.2, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  m\nt = 29.886, df = 122, p-value = 1\nalternative hypothesis: true mean is less than 39.2\n95 percent confidence interval:\n     -Inf 47.96545\nsample estimates:\nmean of x \n 47.50488 \n```\n\n\n:::\n:::\n\n\nFrom all comparison, the one that rejected more differences was the first (compared to Adelie), so our scientific hypotheses that the carcase might belongs to an Adelie penguins cannot be rejected. All other comparisons, rejected this scientific hypothesis.\n\n:::callout-tip\nExactly the same logic applies to the non-parametric version of the test: **Wilcoxon test**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  filter(species==\"Adelie\") %>% \n  pull(bill_length_mm)-> m # Note that I had to use the fucntion pull to extract a numeric vector rather than a tibble acecpted by t.test before\nwilcox.test(m, mu = 39.2, alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  m\nV = 4447, p-value = 0.04137\nalternative hypothesis: true location is not equal to 39.2\n```\n\n\n:::\n\n```{.r .cell-code}\nwilcox.test(m, mu = 39.2, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  m\nV = 4447, p-value = 0.9794\nalternative hypothesis: true location is greater than 39.2\n```\n\n\n:::\n\n```{.r .cell-code}\nwilcox.test(m, mu = 39.2, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  m\nV = 4447, p-value = 0.02069\nalternative hypothesis: true location is less than 39.2\n```\n\n\n:::\n:::\n\n\n:::\n\n### Two(or more)-sample comparisons (independent groups)\n\nThis test is used to compare the means of two independent groups. For instance, it can compare the average weights of individuals grouped by gender. There are two primary forms:\n\n• **Student's t-test**: Assumes that the variance of the two groups are equal.\n\n• **Welch's t-test**: Is less restrictive and does not assume equal variances between the two groups. By default, R typically computes the Welch's t-test, which is considered the safer option, especially when group sizes and standard deviations are very different.\n* **Analyses of Variance**: Works similarly as the t-test but is able to compare more than two groups and uses a different calculation to decide on whether or not to reject null hypothesis, via **F-statisticis**\n\n**Assumptions**: \n\n- Independence of observations: Each subject should belong to only one group, and there should be no relationship between observations across groups.\n\n- No significant outliers in either group.\n\n- Normality: The data for each group should be approximately normally distributed.\n\n- Homogeneity of variances: The variance of the outcome variable should be equal in each group. This assumption is specifically for the standard Student's t-test and is relaxed in the Welch's t-test. Homogeneity of variances can be checked using Levene's test. If variances are unequal, Welch's t-test is often preferred as it maintains a more accurate Type I error rate\n\n**Non-Parametric Alternatives**\n\nWhen the assumptions for parametric tests (like t-tests or ANOVA) are not met, particularly normality or homogeneity of variances, non-parametric alternatives are recommended.\n\n• **Wilcoxon Rank Sum Test (Mann-Whitney U test)**: This is the non-parametric alternative to the independent samples t-test. It determines if two numeric samples are from the same distribution when populations are not normally distributed or have unequal variance, focusing on comparing medians rather than means.\n\n• **Kruskal-Wallis Test**: This is the non-parametric alternative to one-way ANOVA. It tests for differences in the distribution of more than two samples when normality cannot be assumed. Like ANOVA, it is an \"omnibus\" test and typically requires pairwise Wilcoxon tests as post-hoc analyses to identify specific group differences.\n\n## Compare two groups\nLet's consider again the Penguins dataset. Can I use the bill length as helpful character to separate species?\n\nLet's see if Chinstrap and Gentoo species differ in bill length\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Method 1\n\npenguins %>% \n  filter(species==\"Chinstrap\") %>% \n  pull(bill_length_mm)->x\n\npenguins %>% \n  filter(species==\"Gentoo\") %>% \n  pull(bill_length_mm)->y\n\nt.test(x, y, var.equal = TRUE, alternative=\"two.sided\") # testing if they are different\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  x and y\nt = 2.7694, df = 189, p-value = 0.006176\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3823625 2.2755285\nsample estimates:\nmean of x mean of y \n 48.83382  47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(x, y, alternative=\"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  x and y\nt = 2.706, df = 129.22, p-value = 0.003865\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.515293      Inf\nsample estimates:\nmean of x mean of y \n 48.83382  47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(x, y, alternative=\"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  x and y\nt = 2.706, df = 129.22, p-value = 0.9961\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 2.142598\nsample estimates:\nmean of x mean of y \n 48.83382  47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Method 2\n\npenguins %>% \n  filter(species!=\"Adelie\")->peng.t.test \n  t.test(bill_length_mm~species, var.equal=TRUE, data = peng.t.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  bill_length_mm by species\nt = 2.7694, df = 189, p-value = 0.006176\nalternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n95 percent confidence interval:\n 0.3823625 2.2755285\nsample estimates:\nmean in group Chinstrap    mean in group Gentoo \n               48.83382                47.50488 \n```\n\n\n:::\n\n```{.r .cell-code}\n   t.test(bill_length_mm~species, var.equal=FALSE, data = peng.t.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = 2.706, df = 129.22, p-value = 0.00773\nalternative hypothesis: true difference in means between group Chinstrap and group Gentoo is not equal to 0\n95 percent confidence interval:\n 0.3572698 2.3006212\nsample estimates:\nmean in group Chinstrap    mean in group Gentoo \n               48.83382                47.50488 \n```\n\n\n:::\n:::\n\n\n## Compare more than two groups\n\n**ANOVA** is used when you want to compare the means of three or more independent groups on a particular continuous variable. It tells you whether there's an overall statistically significant difference among the group means.\n\nLet's try to determine if there is a statistically significant difference in the mean `bill length` (bill_length_mm) among the different penguin species.\n\n- Categorical Independent Variable (Factor): species (Adelie, Chinstrap, Gentoo) - 3 levels/groups\n\n- Continuous Dependent Variable: bill_length_mm\n\n## State the Hypotheses\n\n- Null Hypothesis (H0): The mean bill length is the same across all penguin species (μ_Adelie = μ_Chinstrap = μ_Gentoo)\n- Alternative Hypothesis (Ha): At least one penguin species has a different mean bill length from the others. (Not all means are equal)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_result <- aov(bill_length_mm ~ species, data = penguins)\nsummary(anova_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       2   7194    3597   410.6 <2e-16 ***\nResiduals   339   2970       9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n```\n\n\n:::\n:::\n\n\n**Visualise the data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  ggplot(aes(species, bill_length_mm, color=species, fill=species))+\n  geom_jitter() +\n  geom_boxplot(alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](week6_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can see visually that Adelie penguins have shorter bills, but how do we get a statistic confirmation on who differs from whom?\n\n**Post-hoc pairwise comparisons**\n\nIn this case, let's apply one of the most common tests, the `Tukey Test` for pairwise comparisons.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(anova_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = bill_length_mm ~ species, data = penguins)\n\n$species\n                      diff       lwr        upr     p adj\nChinstrap-Adelie 10.042433  9.024859 11.0600064 0.0000000\nGentoo-Adelie     8.713487  7.867194  9.5597807 0.0000000\nGentoo-Chinstrap -1.328945 -2.381868 -0.2760231 0.0088993\n```\n\n\n:::\n:::\n\n\n\nEasy to see now, that actually, all species differ in their bill length and there were hidden differences that a simple visual inspection is not able to grasp.\n\n## Summary of Statistical Tests for Mean Differences\n\n: **Table 1: Summary of Comparisons of Statistical Tests for Analysing Mean Differences** {#tbl-statistical-tests}\n\n| Feature                   | T-test (Parametric)                                                                                                                  | ANOVA (Parametric)                                                                                                                        | Wilcoxon Test (Non-parametric)                                                                                                              | Kruskal-Wallis Test (Non-parametric)                                                                                                  |\n| :------------------------ | :----------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------ |\n| **Number of Groups**      | **Two groups**, or **one sample vs. a known value**.                                                 | **Three or more groups**.                                                                                           | **Two groups**, or **one sample vs. a known value**.                                                              | **Three or more groups**. When only two samples are used, results are identical to Wilcoxon rank sum test. |\n| **Dependent/Independent** | Both **independent** and **paired** variants exist.                         | Typically **independent groups**.                                                                                                  | Both **independent** (rank sum) and **paired** (signed-rank) variants exist.                          | **Independent groups**.                                                                                                         |\n| **Key Assumption**        | **Normality of data** (or differences for paired) and **homogeneity of variance** (for Student's). No significant outliers. Independence of observations for independent samples. | **Normality of data** and **homogeneity of variance**. Data independence.                                         | **No specific distribution assumption** (non-parametric).                                                                       | **No specific distribution assumption** (non-parametric).                                                                       |\n| **What it Compares**      | **Means**.                                                                                              | **Means**.                                                                                                                 | **Medians or distributions**.                                                                                                 | **Medians or distributions**.                                                                                           |\n| **R Functions (Base)**    | `t.test()`.                                                                            | `aov()`.                                                                                                                       | `wilcox.test()`.                                                                                                               | `kruskal.test()`.                                                                                                        |\n| **R Packages (Others)**   | `rstatix::t_test()`, `gtsummary::add_p()`, `report::report()`.                                                        | `gtsummary::add_p()`, `report::report()`.                                                                                  | `rstatix::wilcox_test()`, `gtsummary::add_p()`.                                                                       | `rstatix::kruskal_test()`, `gtsummary::add_p()`.                                                                |\n| **Post-hoc Test Needed?** | No (compares only two groups).                                                                                                  | Yes, if omnibus test is significant (`pairwise.t.test()` or `TukeyHSD()`).                            | No (compares only two groups).                                                                                                       | Yes, if significant (`pairwise.wilcox.test()`).                                                                       |\n| **Robustness**            | **Welch's is robust to unequal variances**. Generally robust to normality violations.             | Less robust to unequal variances with unbalanced sample sizes.                                                                     | Generally **more robust to assumption violations**.                                                                                 | Generally **more robust to assumption violations**. Can be unstable with heteroscedasticity.                           |\n| **Relationship**          | Special case of ANOVA when comparing two groups with equal variances. Subsumed under the General Linear Model.           | Generalisation of the t-test; related to linear regression. Subsumed under the General Linear Model.                 | Non-parametric alternative to t-test.                                                                                      | Non-parametric alternative to one-way ANOVA; extension of Wilcoxon.                                                     |\n\n\n\n## Exercises\n\n### Experimental Desing\n\n1- \n\n\n",
    "supporting": [
      "week6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}